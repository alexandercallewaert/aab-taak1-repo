---
title: "Assignment 1 - Report"
author: "Group 29"
date: "4/12/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/github/aab-taak1-repo")
rm(list=ls())
```

## Introduction

This document can be used to add notes to codes and write down our insights on the output we get.

## Loading data

```{r}
library(tidyverse)     ## install.packages("tidyverse")
library(readr)
library(dplyr)
library(knitr)

library(mice)         ## to impute missing values 
library(randomForest)
```

```{r, message=FALSE, warning=FALSE}
## read train data csv. Note the readr:: prefix identifies which package it's in
# TRAIN <- readr::read_csv2("data/train.csv") contains only 78 original variables
load("data/train.RData") # contains the date values in separated columns 
attach(train)
train_origineel <- train

load("data/test.RData")
test_origineel <- test
```

"train.RData" contains the date variables in different columns and the variables are factorised and made numeric (where TRAIN does not). This dataset is obtained with the "MAKE_2_DATASETS.R" file. The test data is also modified in this R script. It's easy to load both datasets into the R environment with the load command.  

Now we're going to look at the missing values for each variable. 

## Preprocessing

```{r}
## nutteloze kolommen weg
train <- train %>%
  select(!(claim_id))

claim_ID_test <- test[,1]

train <- train %>%
  select(!(claim_amount))
  
## Opmerking 3 - instanties verwijderen waarvoor driver_vehicle_id niet overeenkomt met claim_vehicle_id (data quality problemen), hoe ga je dit aanpakken in de test data? Stel dat dit voorkomt en we toch een voorspelling moeten kunnen geven samen met ID van die instantie... 
train <- train %>% 
  filter(as.character(train$driver_vehicle_id) == as.character(train$claim_vehicle_id))
```

Misschien wel eens onderzoeken hoeveel fraude zaken we met die laatste filter verwijderen... Indien dit wel significant is kan het al zijn dat als die twee niet kloppen we met fraude te maken hebben... (probleem is wel dat deze een categorische variabele met veel factoren is en dus al sowieso niet opgenomen wordt door random forest model)

Let's start by seeing how much missing data there is per feature. Usually a safe maximum threshold is 5% of the total for large datasets. If missing data for a certain feature or sample is more than 5% then we probably should leave that feature or sample out. We therefore check for features (columns) and samples (rows) where more than 5% of the data is missing using a simple function

```{r}
miss <- function(x) # to compute NA% of variables 
  {sum(is.na(x))/length(x)*100}

compute_NA_matrix <- function(x)  # matrix of NA%
  {y <- as.matrix(round(apply(x, 2, miss), 6), dimnames = list(c(colnames(x)), c("NA")))
  colnames(y) <- c("Percentage NA")
  return(y)}

NA_variables_threshold <- function(dataset, threshold) # list with variables with NA% > threshold
  {NA_list <- c()
  NA_matrix <- compute_NA_matrix(dataset)
  variables <- rownames(NA_matrix)
  
  for (i in 1:nrow(NA_matrix))
  {newElement <- variables[i]
  if  (NA_matrix[i,1] <= threshold) 
      {NA_list <- c(NA_list, newElement)}}
  return(NA_list)}

compute_NA_thresholdmatrix <- function(dataset, threshold) # matrix with variables with NA% > threshold
  {dataset <- dataset %>% select(all_of(NA_variables_threshold(dataset, threshold)))
  return(compute_NA_matrix(dataset))}

compute_imp_variables <- function(dataset, threshold) # variables that have to be imputed! 
  {NA_variables <- NA_variables_threshold(dataset, threshold) 
  dataset <- dataset %>% select(all_of(NA_variables))
  
  NA_matrix <- compute_NA_matrix(dataset)
  to_impute_variables <- (NA_matrix[,1] != 0)
  NA_matrix <- data.frame(NA_matrix[to_impute_variables,])
  return(NA_matrix)}

compute_NA_matrix(train_origineel)      # Geeft oorspronkelijke % of missing values for all variables
compute_NA_matrix(test_origineel)

compute_NA_thresholdmatrix(train, 20)   # Geeft % of missing values for variables for which threshold hold
compute_NA_thresholdmatrix(test, 20)

compute_imp_variables(train, 20)        # Geeft een matrix van variabelen die ge誰mputeerd moeten worden
compute_imp_variables(test, 20)         # volgens een bepaalde threshold! 
```

This way we select all variables for which the threshold is bigger than the percentage of missing values. We put the variables for which this is true in a list and with this list we select the columns of the train dataset to do the predictive learning. 

```{r}
train <- train %>% 
  select(all_of(NA_variables_threshold(train, 20)))

imp_variables <- rownames(compute_imp_variables(train, 20))
all_variables <- NA_variables_threshold(train, 20) # alle variabelen die we opnemen in model! 

test <- test %>% 
  select(all_of(all_variables[-1]))

compute_NA_matrix(test)
# We gaan enkel met dezelfde variabelen werken in test als in train! 

most_fraudulent <- train %>% 
  filter(fraud == "Y") %>% 
  arrange(desc(claim_amount))
sum(most_fraudulent[1:100, "claim_amount"])
```

Even kijken wat som van 100 grootste fraudulente claims is om een idee te hebben van de grootte-orde. 

## Exploratory analysis

Understanding features

### Bar plots
```{r}
attach(train)
ggplot(data = train) +
  geom_bar(mapping = aes(x = fraud))
## Massive class imbalance

ggplot(data = train) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 500)
ggplot(data = train) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 50)
## There seem to be a distribution of claim amounts around 1000 and outliers located above 15 000

ggplot(data = train) +
  geom_bar(mapping = aes(x = claim_cause))
## Dominant cause of claim registration is traffic_accident
```

### Facets & Frequency polygons

```{r}
ggplot(data = train) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 500)+
  facet_wrap(~fraud)

ggplot(data = train, mapping = aes(x = claim_amount, colour = fraud)) +
  geom_freqpoly(binwidth = 500)
## It seems like there is an equal distribution no matter whether fraud or not
## Trying to zoom in on fraud == "Y" case using most_fraudulent data frame

ggplot(data = most_fraudulent) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 500)
ggplot(data = most_fraudulent) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 100)
## Generally speaking, same distribituion; but also intermediate cases between 1000 and 15000

ggplot(data = most_fraudulent, mapping = aes(x = claim_amount, colour = claim_cause)) +
  geom_freqpoly(binwidth = 400)
```

### Covariation

```{r}
ggplot(data = most_fraudulent, mapping = aes(x = claim_num_vehicles, y = claim_amount)) + 
  geom_boxplot(mapping = aes(group = cut_width(claim_num_vehicles, 1)))
## This is interesting!
## For fraudulent cases, we see that there is some exceptionally high claim amounts when one vehicle is involved
```

### Correlation

```{r}
library(corrplot)
```

## Imputing missing values 

Zowel de train set als de originele test set moeten behandeld worden voor de missing values. Bij de test set zal onze random forest anders ook NA kunnen voorspellen (denk ik momenteel omdat er NA values aanwezig waren bij predict). 

```{r}
compute_NA_matrix(train) # alle variabelen met hun %, ook die 0% zijn
compute_imp_variables(train, 20) # zonder de 0%, dit zijn dus alle variabelen die ge誰mputeerd moeten worden
```

Now, the imputing. You have a lot of different packages to do this! 

### Which variables to impute? 

```{r}
library(mice)         ## to impute missing values 

imputing_var_train <- rownames(compute_imp_variables(train, 20))
imputing_var_test <- rownames(compute_imp_variables(test, 20))

NA_matrix_train <- compute_NA_matrix(train) 
NA_matrix_TEST <- compute_NA_matrix(test)

variables_train <- rownames(NA_matrix_train)
variables_test <- rownames(NA_matrix_TEST)

## Look at the variables with missing values 

row_imp_var_train  <- which(NA_matrix_train[,1] != 0) # list of variables with missing values
name_imp_var_train <- names(row_imp_var_train)     # name of variable in train for which we impute
vctr_imp_var_train <- as.vector(row_imp_var_train) # number original column in train for which we impute 
imp_var_train <- cbind(name_imp_var_train, vctr_imp_var_train)
head(imp_var_train)

row_imp_var_test  <- which(NA_matrix_TEST[,1] != 0) # list of variables with missing values
name_imp_var_test <- names(row_imp_var_test)     # name of variable in test for which we impute
vctr_imp_var_test <- as.vector(row_imp_var_test) # number original column in test for which we impute 
imp_var_test <- cbind(name_imp_var_test, vctr_imp_var_test)
head(imp_var_test)
```

### Imputing the train & test data

We gaan beginnen met 2 methodes om te imputeren! Imputation at level-2 by Predictive mean matching (any), duurt lang om te berekenen, daarom dat ik de ingevulde datasets (zie verder: train1, train2, test1, test2) als files op sla, zodat je ze snel opnieuw kunt laden indien nodig, wel opletten als je ze verandert in je code en je oude files blijft gebruiken! ... 

```{r}
library(mice)
methods(mice)

# Heel deze chunk laden duurt makkelijk 15 uur. 's Avonds om 23u laten starten, om 9u volgende ochtend was test_impute1 volledig afgerond, maar test_impute2 nog maar net klaar met eerste iteratie (maxit = 10). Best deze chunk niet laden, maar gewoon de datasets op het einde van volgende chunk inladen en daar mee werken. 

# Ik veronderstel ook dat deze ge誰mputeerde waarden van goeie kwaliteit zijn om later mee verder te werken voor eender welk predictive learning model. We kunnen ook kolommen van ge誰mputeerde waarden voor de train1 vervangen door kolommen van train2 en ook voor test. Dit gaat allemaal nog moeten gebeuren bij het fine tunen van een model! 
train_impute1 <- mice(train[, vctr_imp_var_train], m = 1, 
                      method = "pmm", 
                      maxit = 10) 

train_impute2 <- mice(train[, vctr_imp_var_train], m = 1, 
                      method = "rf", 
                      maxit = 10) 
# Duurt zeer lang, maar nog langer voor de testset! 

test_impute1 <- mice(test[, vctr_imp_var_test], m = 1, 
                      method = "pmm", 
                      maxit = 10) 

test_impute2 <- mice(test[, vctr_imp_var_test], m = 1, 
                      method = "rf", 
                      maxit = 10) 

# Zodra je dit geladen hebt, duurde enkele uren bij mij, deze ge誰mputeerde waarden invullen in de dataset en vervolgens de datasets opslaan! 
```

### Completing the datasets (train & test)

```{r}
complete_data <- function(dataset, impute, imputing_var, name_imp_var, vctr_imp_var) 
  # complete data with imputed values
  {
  DATASET <- dataset
  
  for (i in 1:nrow(imputing_var))
    {
      imp_variable <- data.frame(impute$imp[name_imp_var[i]])
      matrix_imp_variable <- cbind(rownames(imp_variable), imp_variable)
    
    for (j in 1:nrow(matrix_imp_variable)) # loop for every value of one imputed variable
      {
        ID <- matrix_imp_variable[j,1]   # number of observation in train dataset 
        VALUE <- matrix_imp_variable[j,2] # imputed value
      
        DATASET[ID, vctr_imp_var[i]] <- VALUE} # assign value to [obs. n属, col imputed var]
      }
  return(DATASET)}

train1 <- complete_data(train, train_impute1, imp_var_train, name_imp_var_train, vctr_imp_var_train)
compute_NA_matrix(train1)

train2 <- complete_data(train, train_impute2, imp_var_train, name_imp_var_train, vctr_imp_var_train)
compute_NA_matrix(train2)

test1 <- complete_data(test, test_impute1, imp_var_test, name_imp_var_test, vctr_imp_var_test)
compute_NA_matrix(test1)

## Fout, claim_vehicle_ID is hier helemaal verkeerd ge誰mputeerd, geen idee hoet dit komt...
delete <- which(colnames(test1) == "claim_vehicle_id")
test1 <- test1[,-delete]
compute_NA_matrix(test1)

test2 <- complete_data(test, test_impute2, imp_var_test, name_imp_var_test, vctr_imp_var_test)
compute_NA_matrix(test2)
delete <- which(colnames(test2) == "claim_vehicle_id")
test2 <- test2[,-delete]
compute_NA_matrix(test2)

delete <- which(colnames(train1) == "claim_vehicle_id")
train1 <- train1[,-delete]

delete <- which(colnames(train2) == "claim_vehicle_id")
train2 <- train2[,-delete]

save(train1, file = "train1.RData")
save(train2, file = "train2.RData")
save(test1, file = "test1.RData")
save(test2, file = "test2.RData")
```

## Predictive Learning Model 

### Random Forest model

Error: random forest kan niet werken met factoren waarvan er meer dan 53 levels zijn, dit is een probleem voor: "policy_coverage_type"
```{r}
delete_categorical <- function(x)
  {
    variables <- colnames(x) # names of columns 
    FA_list <- c()
    NA_list <- variables           
    DATASET <- x 

    for (i in 1:ncol(x))     # add those categorical variables to FA_list
      {
        newElement <- variables[i]
        if (class(x[[i]]) == "factor" && nlevels(x[[i]]) > 53) 
            {FA_list <- c(FA_list, newElement)}
      }
    
    for (i in 1:length(FA_list)) # delete elements from NA_list which are in FA_list
      {
        element <- FA_list[i]
        NA_list <- NA_list[NA_list != element]
      }
    
    DATASET <- x %>% select(all_of(NA_list))
    return(DATASET)
  }

train1 <- delete_categorical(train1)
train2 <- delete_categorical(train2)

test1 <- delete_categorical(test1)
test2 <- delete_categorical(test2)
```

Random forest learning model. Wordt aan de hand van een train en test set ge谷valueerd. We splitsen momenteel 80% van de data in een train set en 20% in een test set om daar ons model eens op toe te laten zodat we zeker ook niet overfitten! 

```{r Split Data}
# First we split the train data in test and training! Volgens mij niet nodig voor Random Forest, is gewoon goed om accuracy te meten denk ik!...

prop_train <- 0.8 # proportion of train and test set! 
prop_test <- 1-prop_train 

training_size1 <- floor((prop_train)*nrow(train))
training_size2 <- floor((prop_train)*nrow(train))
set.seed(122000)
# set.seed(34), naargelang deze waarde verschilt kies sample andere random getallen! 

train_index1 <- sample(seq_len(nrow(train)), size = training_size1)
train_index2 <- sample(seq_len(nrow(train)), size = training_size2)

# Train dataset from train data
train1_train      <- train1[train_index1,]
train1_trainLabel <- train1_train[,1]
train2_train      <- train2[train_index2,]
train2_trainLabel <- train2_train[,1]

# Test datasets from train data!!
train1_test       <- train1[-train_index1,]
train1_testLabel  <- train1_test[,1]
train2_test       <- train2[-train_index2,]
train2_testLabel  <- train2_test[,1]
```

```{r}
## Random forest op de train set van de train data! Hier kan je beginnen spelen en zien wat het beste resultaat oplevert...

accuracy_predicted <- function(RF_model, test_set)
  {
  predicted <- predict(RF_model, test_set)
  tab <- table(test_set$fraud, predicted)
  err <- round((1-sum(diag(tab1))/sum(tab1)), 4)
  tab <- cbind(tab, err)
  colnames(tab) <- c("NO", "YES", "ACCURACY")
  return(tab)  
}

create_csv <- function(RF_model, test_data)
  {
  predicted_values <- predict(RF_model, newdata = test_data, type = "prob")
  probabilities <- predicted_values[,2]
  
  csv_file <- cbind(claim_ID_test, probabilities)
  return(csv_file)
  }
```

### Building the models! 

Attempt 1... Score = 288629.61, AUC = 0.68532983
```{r}
RFmodel1 <- randomForest(
  train1_train$fraud~.,
  data = train1_train,
  ntree = 100)                  # Random Forest model of entry 1 online! 

accuracy_predicted(RFmodel1, train1_test)
csv_entry1 <- create_csv(RFmodel1, test1)
write.csv(csv_entry1, "CSV_entry1.csv", row.names = FALSE)
```

Attempt 2... De table ziet er al niet goed uit... Hier wordt 
```{r}
RFmodel <- randomForest(
  train2_train$fraud~.,
  data = train2_train,
  ntree = 100)

accuracy_predicted(RFmodel, train2_test)
csv_entry <- create_csv(RFmodel, test2)
write.csv(csv_entry, "CSV_entry.csv", row.names = FALSE)
```

Attempt 2. No splitting for test and train set and more numbers of trees in random forest learning model! Score = 321323.1, AUC = 0.72061521
```{r}
RFmodel <- randomForest(
  train1$fraud~.,
  data = train1,
  ntree = 1000)

csv_entry <- create_csv(RFmodel, test1)
write.csv(csv_entry, "CSV_entry2.csv", row.names = FALSE) # entry 2 online!
```

Attempt 3, same as attempt 2, but now we use test2 of the imputed test set... 
Score = 317565.2 and AUC = 0.71105264
```{r}
RFmodel <- randomForest(
  train1$fraud~.,
  data = train1,
  ntree = 1000)

csv_entry <- create_csv(RFmodel, test2)
write.csv(csv_entry, "CSV_entry3.csv", row.names = FALSE) # entry 2 online!
```

Attempt 4, use train2 to learn random forest model! Score = 328096.4 and AUC 0.71132037
```{r}
RFmodel <- randomForest(
  train2$fraud~.,
  data = train2,
  ntree = 500)

csv_entry <- create_csv(RFmodel, test2)
write.csv(csv_entry, "CSV_entry4.csv", row.names = FALSE) # entry 4 online!
```

Attempt 5, more number of trees for the previous model and other test set! Score = 321323.1
and AUC = 0.71659454
```{r}
RFmodel <- randomForest(
  train2$fraud~.,
  data = train2,
  ntree = 1000)

csv_entry <- create_csv(RFmodel, test1)
write.csv(csv_entry, "CSV_entry5.csv", row.names = FALSE)
```

Attempt 6, score = 325755.16 and AUC = 0.71442148

```{r}
RFmodel <- randomForest(
  train2$fraud~.,
  data = train2,
  ntree = 1000)

csv_entry <- create_csv(RFmodel, test2)
write.csv(csv_entry, "CSV_entry6.csv", row.names = FALSE)
```


