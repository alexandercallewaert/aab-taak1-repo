---
title: "Assignment 1 - Report"
author: "Group 29"
date: "4/12/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
```

## Introduction

This document can be used to add notes to codes and write down our insights on the output we get.

## Loading data

```{r}
library(tidyverse)     ## install.packages("tidyverse")
library(readr)
library(dplyr)
library(knitr)

library(mice)         ## to impute missing values 

```

```{r, message=FALSE, warning=FALSE}
## read train data csv. Note the readr:: prefix identifies which package it's in
TRAIN <- readr::read_csv2("data/train.csv") # contains only 78 original variables
load("train.RData") # contains the date values in separated columns 
attach(train)

load("test.RData")
```

"train.RData" contains the date variables in different columns and the variables are factorised and made numeric (where TRAIN does not). This dataset is obtained with the "MAKE_2_DATASETS.R" file. The test data is also modified in this R script. It's easy to load both datasets into the R environment with the load command.  

Now we're going to look at the missing values for each variable. 

## Preprocessing

Let's start by seeing how much missing data there is per feature. Usually a safe maximum threshold is 5% of the total for large datasets. If missing data for a certain feature or sample is more than 5% then we probably should leave that feature or sample out. We therefore check for features (columns) and samples (rows) where more than 5% of the data is missing using a simple function

```{r}
miss <- function(x){sum(is.na(x))/length(x)*100}
apply(train, 2, miss)
apply(train, 1, miss) # Waarom deze command? ~Rosseel

NA_matrix <- as.matrix(round(apply(train, 2, miss), 2), dimnames = list(c(colnames(train)), c("NA"))) 
colnames(NA_matrix) <- c("Percentage NA")

variables <- rownames(NA_matrix)
NA_list <- c()
NA_threshold <- 20

for (i in 1:nrow(NA_matrix))
  {
  newElement <- variables[i]
  if (NA_matrix[i,1] <= NA_threshold) {NA_list <- c(NA_list, newElement)}
  }
```

This way we select all variables for which the threshold is bigger than the percentage of missing values. We put the variables for which this is true in a list and with this list we select the columns of the train dataset (see below: "train_NA_threshold") to do the predictive learning. 

```{r}
train_NA_threshold <- train %>% select(all_of(NA_list))

most_fraudulent <- train_NA_threshold %>% 
  filter(fraud == "Y") %>% 
  arrange(desc(claim_amount))
sum(most_fraudulent[1:100, "claim_amount"])
```

Even kijken wat som van 100 grootste fraudulente claims is om een idee te hebben van de grootte-orde

```{r}
## nutteloze kolommen weg
train_ready2 <- train_NA_threshold %>%
  select(!(claim_id))

# train_ready2 <- train_ready2 %>%
  # select(!(claim_amount))
  
## Opmerking 3 - instanties verwijderen waarvoor driver_vehicle_id niet overeenkomt met claim_vehicle_id (data quality problemen), hoe ga je dit aanpakken in de test data? Stel dat dit voorkomt en we toch een voorspelling moeten kunnen geven samen met ID van die instantie... 
train_ready2 <- train_ready2 %>% 
  filter(as.character(train_ready2$driver_vehicle_id) == as.character(train_ready2$claim_vehicle_id))
```

## Exploratory analysis

Understanding features

### Bar plots
```{r}
attach(train_ready2)
ggplot(data = train_ready2) +
  geom_bar(mapping = aes(x = fraud))
## Massive class imbalance

ggplot(data = train_ready2) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 500)
ggplot(data = train_ready2) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 50)
## There seem to be a distribution of claim amounts around 1000 and outliers located above 15 000

ggplot(data = train_ready2) +
  geom_bar(mapping = aes(x = claim_cause))
## Dominant cause of claim registration is traffic_accident
```

### Facets & Frequency polygons

```{r}
ggplot(data = train_ready2) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 500)+
  facet_wrap(~fraud)

ggplot(data = train_ready2, mapping = aes(x = claim_amount, colour = fraud)) +
  geom_freqpoly(binwidth = 500)
## It seems like there is an equal distribution no matter whether fraud or not
## Trying to zoom in on fraud == "Y" case using most_fraudulent data frame

ggplot(data = most_fraudulent) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 500)
ggplot(data = most_fraudulent) +
  geom_histogram(mapping = aes(x = claim_amount), binwidth = 100)
## Generally speaking, same distribituion; but also intermediate cases between 1000 and 15000

ggplot(data = most_fraudulent, mapping = aes(x = claim_amount, colour = claim_cause)) +
  geom_freqpoly(binwidth = 400)
```

### Covariation

```{r}
ggplot(data = most_fraudulent, mapping = aes(x = claim_num_vehicles, y = claim_amount)) + 
  geom_boxplot(mapping = aes(group = cut_width(claim_num_vehicles, 1)))
## This is interesting!
## For fraudulent cases, we see that there is some exceptionally high claim amounts when one vehicle is involved
```

### Correlation

```{r}
library(corrplot)
```

### Imputing missing values 

```{r}
# Werk verder met train_ready2, deze dataset is klaar voor learning model, enkel nog missing values behandelen. 

miss <- function(x){sum(is.na(x))/length(x)*100}
apply(train_ready2, 2, miss) # Missing values zijn al fel verminderd door filters 

NA_matrix <- as.matrix(round(apply(train_ready2, 2, miss), 5), 
                       dimnames = list(c(colnames(train_ready2)), c("Missing Values"))) 
colnames(NA_matrix) <- c("Percentage NA")

kable(NA_matrix, head = "% of Missing Values")
NA_variables <- which(NA_matrix[,1] != 0.00000) # list of variables with missing values
NA_variables
```

Now, the imputing. You have a lot of different packages to do this! 

```{r}
library(mice)         ## to impute missing values 

load("train.RData") # contains the date values in separated columns 
attach(train)

train_original <- train

miss <- function(x)   ## compute NA%
  {sum(is.na(x))/length(x)*100}

compute_NA_matrix <- function(x)  ## matrix of NA%
  {y <- as.matrix(round(apply(x, 2, miss), 2), dimnames = list(c(colnames(x)), c("NA")))
  colnames(y) <- c("Percentage NA")
  return(y)}

compute_NA_variables <- function(matrix, threshold, variables)
  {NA_list <- c()
  for (i in 1:nrow(matrix))
    {
    newElement <- variables[i]
    if (NA_matrix[i,1] <= threshold) {NA_list <- c(NA_list, newElement)}
    }
  return(NA_list)
}

NA_matrix <- compute_NA_matrix(train) 
ALL_variables <- rownames(NA_matrix)
NA_threshold <- 20

# berekenen van welke variabelen voldoen aan NA_threshold en in lijst steken, op basis van 
# deze lijst selecteren we de kolommen uit de train matrix! 
NA_list <- compute_NA_variables(NA_matrix, NA_threshold, ALL_variables)

train <- train %>% select(all_of(NA_list)) ## Enkel de variabelen waarvoor NA% < 20%

train <- train %>%      # geen claim_id aanwezig...
  select(!(claim_id))

train <- train %>%      # driver_vehicle_id != claim_vehicle_id, verwijderen! 
  filter(as.character(train$driver_vehicle_id) == as.character(train$claim_vehicle_id))


## Look at the variables with missing values 

NA_matrix <- compute_NA_matrix(train)
NA_matrix 

imputing_var <- which(NA_matrix[,1] != 0.00000) # list of variables with missing values

names_imputing_var  <- names(imputing_var) # name of variable in train for which we impute
vector_imputing_var <- as.vector(imputing_var) # number of original column in train for which we impute 
imputing_var <- cbind(names_imputing_var, vector_imputing_var)


###### Imputing 

train_impute <- mice(train[, vector_imputing_var], m = 1, method = "pmm", maxit=10)
train_impute$imp # all rownumbers and their imputed value for specific variable

## Complete train matrix with the imputed values for variables 

for (i in 1:nrow(imputing_var)) # loop for every imputed variable 
  {
  imp_variable <- data.frame(train_impute$imp[names_imputing_var[i]])
  matrix_imp_variable <- cbind(rownames(imp_variable), imp_variable) # makkelijker om hiermee 
                                                                     # waarden toe te wijzen
  for (j in 1:nrow(matrix_imp_variable)) # loop for every value of one imputed variable
    {ID <- matrix_imp_variable[j,1]   # number of observation in train dataset 
    VALUE <- matrix_imp_variable[j,2] # imputed value
    
    train[ID,vector_imputing_var[i]] <- VALUE} # assign value to [observation nÂ°, original column imputed variable]
  }

NA_matrix <- compute_NA_matrix(train)
NA_matrix # all %NA = 0%, good! 

# Error: random forest kan niet werken met factoren waarvan er meer dan 53 levels zijn, 
# dit is een probleem voor: "policy_coverage_type"
str(train)

train1 <- train

for (i in 1:ncol(train))
  {if (class(train[[i]]) == "factor" && nlevels(train[[i]]) > 53) {train1 <- train[,-i]}}

variables <- colnames(train)
FA_list <- c()

for (i in 1:ncol(train))
  {
  newElement <- variables[i]
  if (class(train[[i]]) == "factor" && nlevels(train[[i]]) > 53) {FA_list <- c(FA_list, newElement)}
  }

train <- train %>% select(all_of(FA_list))

round(apply(train, 2, miss), 5)


rf <- randomForest(
  train$fraud ~., 
  data=train,
  na.action = na.roughfix)
    
```

train_mice[NA_variables[2]]



## Predictive Learning Model 


```{r}

```




